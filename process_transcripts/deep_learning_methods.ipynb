{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "transcript_ml_research.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C32qg7AOG1KO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://files.deeppavlov.ai/deeppavlov_data/bert/ru_conversational_cased_L-12_H-768_A-12_pt.tar.gz\n",
        "!tar -xzf ru_conversational_cased_L-12_H-768_A-12_pt.tar.gz\n",
        "!pip install youtube_transcript_api deeppavlov pytorch_pretrained_bert transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRnVdOUoXJO4",
        "colab_type": "code",
        "outputId": "35e1c947-c150-4986-ff56-7b2eb2df53fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch \n",
        "import nltk\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from pathlib import Path\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from deeppavlov.core.common.file import read_json\n",
        "from deeppavlov import build_model, configs\n",
        "from transformers import AutoModel\n",
        "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append('/content/drive/My Drive/youtube_timestamps')\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXQsp0zQpO--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BERT_MODEL_PATH = '/content/ru_conversational_cased_L-12_H-768_A-12_pt'\n",
        "ROOT_DATA_PATH = Path('/content/drive/My Drive/youtube_timestamps/data')\n",
        "TRAIN_HOLDOUT_RATIO = 0.8      # split dataframe to involved and holdout parts\n",
        "TRAIN_VALIDATE_RATIO = 0.8     # split involved to train and validate parts\n",
        "CONTEXT_RADIUS = 3\n",
        "CONTEXT_SIZE = CONTEXT_RADIUS * 2 + 1\n",
        "BATCH_SIZE = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqDFVL4NfJ68",
        "colab_type": "code",
        "outputId": "5d8d6d9e-ddd5-495d-c40f-2eab91d9e3e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH)\n",
        "bert_model = BertModel.from_pretrained(BERT_MODEL_PATH)\n",
        "bert_model.eval()\n",
        "bert_model.to('cuda')\n",
        "print('RuConversationalBERT loaded to device')"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RuConversationalBERT loaded to device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HooInUkj6rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eliminate_stop_words(sentence):\n",
        "    tokens = [word \n",
        "              for word in nltk.word_tokenize(sentence) \n",
        "              if word not in stopwords.words('russian')]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def get_embedding(sentence, nlp_model, nlp_tokenizer):\n",
        "    if sentence is None or sentence == '':\n",
        "        return torch.zeros(1, nlp_model.config.hidden_size)\n",
        "    tokens = nlp_tokenizer.tokenize(sentence)\n",
        "    indexed_tokens = nlp_tokenizer.convert_tokens_to_ids(tokens)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens]).to('cuda')\n",
        "    with torch.no_grad():\n",
        "        outputs = nlp_model.forward(tokens_tensor)[1].to('cpu')\n",
        "    torch.cuda.empty_cache()\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH7PaQgNueaR",
        "colab_type": "code",
        "outputId": "4730b209-458a-4c0a-9dbf-921355337e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "df = pd.read_csv(ROOT_DATA_PATH / 'zhiza_timestamps.csv')\n",
        "df['pause'] = df['time_start'].shift(-1) - (df['time_start'] + df['time_duration'])\n",
        "df['text'] = df['text'].apply(eliminate_stop_words)\n",
        "df.drop('desc', axis=1, inplace=True)\n",
        "df.fillna(0, inplace=True)\n",
        "df.head()"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video_id</th>\n",
              "      <th>sentence_num</th>\n",
              "      <th>text</th>\n",
              "      <th>time_start</th>\n",
              "      <th>time_duration</th>\n",
              "      <th>is_timestamp</th>\n",
              "      <th>pause</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BCIurE0kubE</td>\n",
              "      <td>0</td>\n",
              "      <td>изучила страну 1000 городах</td>\n",
              "      <td>4.13</td>\n",
              "      <td>6.670</td>\n",
              "      <td>0</td>\n",
              "      <td>-3.030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BCIurE0kubE</td>\n",
              "      <td>1</td>\n",
              "      <td>побывала пил чая поезде</td>\n",
              "      <td>7.77</td>\n",
              "      <td>5.100</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BCIurE0kubE</td>\n",
              "      <td>2</td>\n",
              "      <td>подстаканника прочувствовал</td>\n",
              "      <td>10.80</td>\n",
              "      <td>4.410</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>BCIurE0kubE</td>\n",
              "      <td>3</td>\n",
              "      <td>прелесть железных дорог плацкартом жизнь</td>\n",
              "      <td>12.87</td>\n",
              "      <td>5.669</td>\n",
              "      <td>0</td>\n",
              "      <td>-3.329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>BCIurE0kubE</td>\n",
              "      <td>4</td>\n",
              "      <td>бурлит здорово общается</td>\n",
              "      <td>15.21</td>\n",
              "      <td>6.300</td>\n",
              "      <td>0</td>\n",
              "      <td>-2.971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      video_id  sentence_num  ... is_timestamp  pause\n",
              "0  BCIurE0kubE             0  ...            0 -3.030\n",
              "1  BCIurE0kubE             1  ...            0 -2.070\n",
              "2  BCIurE0kubE             2  ...            0 -2.340\n",
              "3  BCIurE0kubE             3  ...            0 -3.329\n",
              "4  BCIurE0kubE             4  ...            0 -2.971\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FyTVkTD9rhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "holdout_split_idx = int(df['video_id'].nunique() * TRAIN_HOLDOUT_RATIO)\n",
        "validate_split_idx = int(holdout_split_idx * TRAIN_VALIDATE_RATIO)\n",
        "video_ids = df['video_id'].unique()\n",
        "\n",
        "train_video_ids = video_ids[:validate_split_idx]\n",
        "validate_video_ids = video_ids[validate_split_idx:holdout_split_idx]\n",
        "test_video_ids = video_ids[holdout_split_idx:]\n",
        "\n",
        "train_df = df[df['video_id'].isin(train_video_ids)]\n",
        "validate_df = df[df['video_id'].isin(validate_video_ids)]\n",
        "test_df = df[df['video_id'].isin(test_video_ids)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2pyUMoUeDlF",
        "colab_type": "code",
        "outputId": "23a9de4a-7429-4eac-94c9-6fe79af83c14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_df.shape, validate_df.shape, test_df.shape"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((17584, 7), (3724, 7), (4033, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUGw5iT6g_8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_transcripts_dataframe(dataframe, nlp_model, nlp_tokenizer, video_ids):\n",
        "    \"\"\"Gets original (dataframe) and (video_ids) array, then \n",
        "    process sentences with (nlp_model), then \n",
        "    separate different videos to different tensor datasets.\n",
        "\n",
        "    Returns list of ``torch.utils.data.TensorDataset`` of float32.\n",
        "    \"\"\"\n",
        "\n",
        "    def convert_dataframe_to_tensor(dataframe):\n",
        "        first_part = torch.tensor(dataframe.loc[:, dataframe.columns != 'text'].values, dtype=torch.float)\n",
        "        embeddings = []\n",
        "        for sentence in dataframe['text'].values:\n",
        "            embeddings.append(get_embedding(sentence, nlp_model, nlp_tokenizer).float())\n",
        "        second_part = torch.cat(embeddings, dim=0)\n",
        "        return torch.cat((first_part, second_part), dim=1)\n",
        "\n",
        "    datasets = []\n",
        "    for video_id in video_ids:\n",
        "        current_df = dataframe[dataframe['video_id'] == video_id]\n",
        "        X_df = current_df.drop(['video_id', 'is_timestamp'], axis=1)\n",
        "        y_df = current_df['is_timestamp']\n",
        "        X_tensor = convert_dataframe_to_tensor(X_df)\n",
        "        y_tensor = torch.tensor(y_df.values, dtype=torch.float)\n",
        "        datasets.append(torch.utils.data.TensorDataset(X_tensor, y_tensor))\n",
        "\n",
        "    return datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjwIhob1s4W9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8a1e08be-3b1d-40f6-f654-db9a381bd312"
      },
      "source": [
        "%%time\n",
        "try:\n",
        "    train_tensor_datasets = torch.load(ROOT_DATA_PATH / 'train_tensor_datasets.pt')\n",
        "    validate_tensor_datasets = torch.load(ROOT_DATA_PATH / 'validate_tensor_datasets.pt')\n",
        "    test_tensor_datasets = torch.load(ROOT_DATA_PATH / 'test_tensor_datasets.pt')\n",
        "except IOError:\n",
        "    train_tensor_datasets = process_transcripts_dataframe(train_df, bert_model, bert_tokenizer, train_video_ids)\n",
        "    validate_tensor_datasets = process_transcripts_dataframe(validate_df, bert_model, bert_tokenizer, validate_video_ids)\n",
        "    test_tensor_datasets = process_transcripts_dataframe(test_df, bert_model, bert_tokenizer, test_video_ids)\n",
        "    torch.save(train_tensor_datasets, ROOT_DATA_PATH / 'train_tensor_datasets.pt')\n",
        "    torch.save(validate_tensor_datasets, ROOT_DATA_PATH / 'validate_tensor_datasets.pt')\n",
        "    torch.save(test_tensor_datasets, ROOT_DATA_PATH / 'test_tensor_datasets.pt')"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.72 ms, sys: 55.4 ms, total: 60.1 ms\n",
            "Wall time: 112 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbPQKjT_1p1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContextSampler(torch.utils.data.BatchSampler):\n",
        "    def __init__(self, sampler, context_size, stride):\n",
        "        super().__init__(sampler, context_size, drop_last=False)\n",
        "        assert stride <= context_size\n",
        "        self.context_size = context_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def __iter__(self):\n",
        "        context = []\n",
        "        for idx in self.sampler:\n",
        "            context.append(idx)\n",
        "            if len(context) == self.context_size:\n",
        "                yield context\n",
        "                context = context[self.stride:]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sampler) - self.batch_size + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGm3XJ-pDRN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ContextTensorsDataset(torch.utils.data.TensorDataset):\n",
        "    def __init__(self, context_loader, context_size):\n",
        "        X, y = [], []\n",
        "        for entry in context_loader:\n",
        "            X.append(entry[0].unsqueeze(0))\n",
        "            y.append(entry[1][context_size // 2].unsqueeze(0))\n",
        "        X_tensor = torch.cat(X, 0)\n",
        "        y_tensor = torch.cat(y, 0)\n",
        "        super().__init__(X_tensor, y_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZKbXdDuwk6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_contexts_based_data_loader(dataset, context_size, batch_size, stride):\n",
        "    \"\"\"Splits (dataset) into contexts-tensors of (context_size) with (stride), \n",
        "    then produce batches of (batch_size), based on randomly permuted contexts.\n",
        "\n",
        "    Returns ``torch.utils.data.DataLoader`` generator object.\n",
        "    \"\"\"\n",
        "    ctx_sampler = ContextSampler(torch.utils.data.SequentialSampler(dataset), context_size, stride)\n",
        "    ctx_loader = torch.utils.data.DataLoader(dataset, batch_sampler=ctx_sampler)\n",
        "    ctx_dataset = ContextTensorsDataset(ctx_loader, context_size)\n",
        "    random_ctx_sampler = torch.utils.data.RandomSampler(ctx_dataset)\n",
        "    batch_sampler = torch.utils.data.BatchSampler(random_ctx_sampler, batch_size, drop_last=False)\n",
        "    data_loader = torch.utils.data.DataLoader(ctx_dataset, batch_sampler=batch_sampler)\n",
        "\n",
        "    return data_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKs5ku0CVT7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Model, self).__init__()\n",
        "        self.context_size = kwargs['context_size']\n",
        "        self.batch_size = kwargs['batch_size']\n",
        "        self.input_length = kwargs['input_length']\n",
        "\n",
        "        self.conv1 = nn.Conv1d(self.context_size, self.context_size // 2, 7)\n",
        "        self.conv2 = nn.Conv1d(self.context_size // 2, 1, 5)\n",
        "        self.max_pool = nn.MaxPool1d(5)\n",
        "\n",
        "        self.fc1 = nn.Linear(int((self.input_length - 15) / 5 + 1), 300)\n",
        "        self.fc2 = nn.Linear(300, 50)\n",
        "        self.out = nn.Linear(50, 1)\n",
        "\n",
        "        self.act = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.batch_norm = nn.BatchNorm1d(self.context_size)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        convoluted = self.max_pool(self.conv2(self.conv1(x)))\n",
        "        hidden = self.act(self.fc2(self.act(self.fc1(convoluted))))\n",
        "        embeds = hidden.view(hidden.shape[0], hidden.shape[1] * hidden.shape[2])\n",
        "        return self.out(embeds).view(embeds.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZrMrOiykcju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mcc(tp, tn, fp, fn):\n",
        "    \"\"\"Matthews correlation coefficient, belongs to [-1, 1]:\n",
        "    \n",
        "    -1 means trash, absolute incorrect prediction\n",
        "    0 corresponds to random prediction\n",
        "    1 means perfect prediction\n",
        "    \"\"\"\n",
        "    return (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlM8QuX5ATML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(context_size=CONTEXT_SIZE, \n",
        "              batch_size=BATCH_SIZE, \n",
        "              input_length=ROW_LENGTH)\n",
        "loss = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBQncMhkIEbe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "d4d9fa74-289e-4d52-d71c-d0d8b669fd94"
      },
      "source": [
        "for epoch in range(37):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for dataset in train_tensor_datasets:\n",
        "        data_loader = get_contexts_based_data_loader(dataset, CONTEXT_SIZE, BATCH_SIZE, stride=1)\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model.forward(X_batch)\n",
        "            loss_val = loss(outputs, y_batch)\n",
        "            loss_val.backward()\n",
        "            total_loss += loss_val\n",
        "            optimizer.step()\n",
        "    print(f'epoch #{epoch} | loss: {total_loss}')"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #0 | loss: 47.97410202026367\n",
            "epoch #1 | loss: 46.91338348388672\n",
            "epoch #2 | loss: 41.32014846801758\n",
            "epoch #3 | loss: 46.23487091064453\n",
            "epoch #4 | loss: 37.94314956665039\n",
            "epoch #5 | loss: 36.361167907714844\n",
            "epoch #6 | loss: 33.61874008178711\n",
            "epoch #7 | loss: 36.96523666381836\n",
            "epoch #8 | loss: 31.8024959564209\n",
            "epoch #9 | loss: 36.27674865722656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enJav1SpWnHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model_on_datasets(model, datasets):\n",
        "    model.eval()\n",
        "    video_num = 0\n",
        "    for dataset in datasets:\n",
        "        data_loader = get_contexts_based_data_loader(dataset, CONTEXT_SIZE, \n",
        "                                                    batch_size=1, stride=CONTEXT_SIZE)\n",
        "        TP, FP, TN, FN = 0, 0, 0, 0\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            outputs = torch.round(torch.sigmoid(model.forward(X_batch)))\n",
        "            if torch.sum(y_batch) > 0 and torch.sum(outputs) > 0:\n",
        "                TP += 1\n",
        "            elif torch.sum(y_batch) == 0 and torch.sum(outputs) == 0:\n",
        "                TN += 1\n",
        "            elif torch.sum(y_batch) == 0 and torch.sum(outputs) > 0:\n",
        "                FP += 1\n",
        "            else:\n",
        "                FN += 1\n",
        "        if TP == FP == FN == 0:\n",
        "            precision, mcc_coef = 1, 1\n",
        "        elif TP + FP != 0:\n",
        "            precision = TP / (TP + FP)\n",
        "            mcc_coef = mcc(TP, TN, FP, FN)\n",
        "        else:\n",
        "            precision, mcc_coef = 0, 0\n",
        "        print(f'video: #{video_num} | precision: {precision:.3f} | mcc: {mcc_coef:.3f}')\n",
        "        video_num += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSABlIARbyjy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "b165b632-e42b-4016-8077-157d4b2ade10"
      },
      "source": [
        "test_model_on_datasets(model, validate_tensor_datasets)"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "video: #0 | precision: 0.000 | mcc: -0.033\n",
            "video: #1 | precision: 0.000 | mcc: -0.088\n",
            "video: #2 | precision: 0.333 | mcc: 0.257\n",
            "video: #3 | precision: 0.000 | mcc: -0.048\n",
            "video: #4 | precision: 0.000 | mcc: 0.000\n",
            "video: #5 | precision: 1.000 | mcc: 1.000\n",
            "video: #6 | precision: 0.250 | mcc: 0.483\n",
            "video: #7 | precision: 0.333 | mcc: 0.549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S8MQfhYrh9O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "8a6e7449-c093-493f-c28f-d967937c91b0"
      },
      "source": [
        "test_model_on_datasets(model, test_tensor_datasets)"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "video: #0 | precision: 1.000 | mcc: 0.810\n",
            "video: #1 | precision: 0.833 | mcc: 0.728\n",
            "video: #2 | precision: 0.500 | mcc: 0.576\n",
            "video: #3 | precision: 0.556 | mcc: 0.725\n",
            "video: #4 | precision: 0.667 | mcc: 0.807\n",
            "video: #5 | precision: 0.167 | mcc: 0.390\n",
            "video: #6 | precision: 0.000 | mcc: nan\n",
            "video: #7 | precision: 0.400 | mcc: 0.614\n",
            "video: #8 | precision: 0.500 | mcc: 0.556\n",
            "video: #9 | precision: 0.333 | mcc: 0.552\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo4gazhErne2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}